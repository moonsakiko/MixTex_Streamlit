# æ–‡ä»¶å: streamlit_app.py

import streamlit as st
from PIL import Image
import numpy as np
import onnxruntime as ort
from transformers import AutoTokenizer, AutoImageProcessor
import re
import io

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(page_title="MixTeX LaTeX OCR åœ¨çº¿å·¥å…·", page_icon="ğŸ“")
st.title("ğŸ“ MixTeX - LaTeX/å…¬å¼/è¡¨æ ¼ OCR")
st.info("ä¸Šä¼ ä¸€å¼ åŒ…å«å…¬å¼ã€è¡¨æ ¼æˆ–ä¸­è‹±æ–‡æ–‡æœ¬çš„å›¾ç‰‡ï¼ŒAIå°†è‡ªåŠ¨è¯†åˆ«å¹¶è¿”å›LaTeXæˆ–çº¯æ–‡æœ¬ç»“æœã€‚")
st.warning("æ¨¡å‹è¾ƒå¤§ï¼Œé¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")

# ===================================================================
# ---                         æ ¸å¿ƒé€»è¾‘éƒ¨åˆ† (å¿ƒè„ç§»æ¤)                 ---
# ===================================================================

@st.cache_resource
def load_model(model_dir="onnx"):
    """
    åŠ è½½æ‰€æœ‰å¿…è¦çš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    æ¨¡å‹æ–‡ä»¶éœ€è¦æ”¾åœ¨ä¸€ä¸ªåä¸º 'onnx' çš„æ–‡ä»¶å¤¹é‡Œã€‚
    """
    try:
        # ç¡®ä¿Hugging Faceçš„ç¼“å­˜ç›®å½•å­˜åœ¨
        # from pathlib import Path
        # Path("/app/.cache/huggingface/hub").mkdir(parents=True, exist_ok=True)

        tokenizer = AutoTokenizer.from_pretrained(model_dir)
        feature_extractor = AutoImageProcessor.from_pretrained(model_dir)
        
        # ä¸ºONNXæ¨¡å‹æŒ‡å®šprovidersï¼Œä¼˜å…ˆä½¿ç”¨CPU
        providers = ['CPUExecutionProvider']
        encoder_sess = ort.InferenceSession(f"{model_dir}/encoder_model.onnx", providers=providers)
        decoder_sess = ort.InferenceSession(f"{model_dir}/decoder_model_merged.onnx", providers=providers)
        
        return tokenizer, feature_extractor, encoder_sess, decoder_sess
    except Exception as e:
        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ï¼Œæ–¹ä¾¿æ’é”™
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·ç¡®ä¿'onnx'æ–‡ä»¶å¤¹åŒ…å«æ‰€æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œå¹¶ä¸”ä½äºé¡¹ç›®æ ¹ç›®å½•ã€‚é”™è¯¯: {e}")
        return None

def pad_image(img, out_size=(448, 448)):
    """å°†å›¾ç‰‡ç¼©æ”¾å¹¶å¡«å……åˆ°ç›®æ ‡å°ºå¯¸"""
    x_img, y_img = out_size
    bg = Image.new("RGB", (x_img, y_img), (255, 255, 255))
    w, h = img.size
    if w > x_img or h > y_img:
        scale = min(x_img / w, y_img / h)
        nw, nh = int(w * scale), int(h * scale)
        img = img.resize((nw, nh), Image.LANCZOS)
    
    w, h = img.size
    x = (x_img - w) // 2
    y = (y_img - h) // 2
    bg.paste(img, (x, y))
    return bg

def check_repetition(s, repeats=12):
    """æ£€æŸ¥ç”Ÿæˆæ–‡æœ¬ä¸­æ˜¯å¦æœ‰è¿‡åº¦é‡å¤"""
    if len(s) < repeats: return False
    for pattern_length in range(1, len(s) // repeats + 1):
        pattern = s[-pattern_length:]
        if s[-repeats * pattern_length:] == pattern * repeats:
            return True
    return False

def stream_inference(image, model, max_length=512, num_layers=6, hidden_size=768, heads=12, batch_size=1):
    """æµå¼æ¨ç†å‡½æ•°ï¼Œé€ä¸ªtokenç”Ÿæˆç»“æœ"""
    tokenizer, feature_extractor, enc_session, dec_session = model
    head_size = hidden_size // heads
    
    inputs = feature_extractor(image, return_tensors="np").pixel_values
    enc_out = enc_session.run(None, {"pixel_values": inputs})[0]
    
    dec_in = {
        "input_ids": tokenizer("<s>", return_tensors="np").input_ids.astype(np.int64),
        "encoder_hidden_states": enc_out,
        "use_cache_branch": np.array([True], dtype=bool),
        **{
            f"past_key_values.{i}.{t}": np.zeros(
                (batch_size, heads, 0, head_size), dtype=np.float32
            )
            for i in range(num_layers)
            for t in ["key", "value"]
        },
    }
    
    generated = ""
    for _ in range(max_length):
        outs = dec_session.run(None, dec_in)
        next_id = np.argmax(outs[0][:, -1, :], axis=-1)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸç¬¦
        if next_id[0] == tokenizer.eos_token_id:
            break
            
        token_text = tokenizer.decode(next_id, skip_special_tokens=True)
        yield token_text
        
        generated += token_text
        if check_repetition(generated, 21):
            break
            
        dec_in.update(
            {
                "input_ids": next_id[:, None],
                **{
                    f"past_key_values.{i}.{t}": outs[i * 2 + 1 + j]
                    for i in range(num_layers)
                    for j, t in enumerate(["key", "value"])
                },
            }
        )

# ===================================================================
# ---                         å‰ç«¯ç•Œé¢éƒ¨åˆ† (æ–°è½¦èº«)                   ---
# ===================================================================

# åŠ è½½æ¨¡å‹ (åªä¼šæ‰§è¡Œä¸€æ¬¡)
model = load_model()

if model:
    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶ (æ”¯æŒPNG, JPG, JPEG)", type=["png", "jpg", "jpeg"])

    if uploaded_file:
        try:
            img = Image.open(uploaded_file).convert("RGB")
            
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("å›¾ç‰‡é¢„è§ˆ")
                st.image(img, caption="ä¸Šä¼ å›¾ç‰‡")
            
            with col2:
                st.subheader("è¯†åˆ«ç»“æœ")
                if st.button("å¼€å§‹è¯†åˆ«", use_container_width=True):
                    with st.spinner("AI æ­£åœ¨è¯†åˆ«ä¸­..."):
                        # é¢„å¤„ç†å›¾ç‰‡
                        img_padded = pad_image(img)
                        
                        # æµå¼è¾“å‡ºç»“æœ
                        partial_result = ""
                        output_area = st.empty()
                        for piece in stream_inference(img_padded, model):
                            partial_result += piece
                            output_area.markdown(f"```latex\n{partial_result}\n```")
                    
                    st.success("è¯†åˆ«å®Œæˆï¼")
        except Exception as e:
            st.error(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
else:
    st.error("æ¨¡å‹æœªèƒ½åŠ è½½ï¼Œåº”ç”¨æ— æ³•è¿è¡Œã€‚è¯·æ£€æŸ¥æ—¥å¿—ã€‚")